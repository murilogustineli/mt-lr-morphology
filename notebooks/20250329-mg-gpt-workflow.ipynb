{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# get API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# init OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "def prompt_gpt(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    temperature: float = 0.0,\n",
    "    max_tokens: int = 200,\n",
    "):\n",
    "    # query ChatGPT\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                # \"content\": f\"You are a helpful assistant designed to answer the user's prompt.\",\n",
    "                \"content\": \"You are a helpful assistant designed to carefully analyze academic abstracts based on specific inclusion and exclusion criteria.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test.\n"
     ]
    }
   ],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "prompt = \"Say this is a test and nothing else.\"\n",
    "response = prompt_gpt(model, prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database</th>\n",
       "      <th>authors</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author keywords</th>\n",
       "      <th>index keywords</th>\n",
       "      <th>low resource</th>\n",
       "      <th>morphology</th>\n",
       "      <th>languages</th>\n",
       "      <th>...</th>\n",
       "      <th>languages old</th>\n",
       "      <th>status</th>\n",
       "      <th>reviewer 1</th>\n",
       "      <th>reviewer 1 verdict</th>\n",
       "      <th>reviewer 1 reason</th>\n",
       "      <th>reviewer 1 notes</th>\n",
       "      <th>reviewer 2</th>\n",
       "      <th>reviewer 2 verdict</th>\n",
       "      <th>reviewer 2 reason</th>\n",
       "      <th>reviewer 2 notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scopus</td>\n",
       "      <td>rikters, matiss (57190290578); pinnis, marcis ...</td>\n",
       "      <td>advancing estonian machine translation</td>\n",
       "      <td>2018</td>\n",
       "      <td>in this paper, we present tildes work on boost...</td>\n",
       "      <td>morphologically rich languages; multi-way mach...</td>\n",
       "      <td>computational linguistics; computer aided lang...</td>\n",
       "      <td>less resource</td>\n",
       "      <td>morpholog</td>\n",
       "      <td>estonian (3)</td>\n",
       "      <td>...</td>\n",
       "      <td>estonian (3)</td>\n",
       "      <td>False</td>\n",
       "      <td>Agustin</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Low-Resource</td>\n",
       "      <td>Estonian is level 3, not 2 or lower, and only ...</td>\n",
       "      <td>Ahmad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acl-anthology</td>\n",
       "      <td>[baliber, renz iver, cheng, charibeth, adlaon,...</td>\n",
       "      <td>bridging philippine languages with multilingua...</td>\n",
       "      <td>2020</td>\n",
       "      <td>the philippines is home to more than 150 langu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>low-resource</td>\n",
       "      <td>morpholog</td>\n",
       "      <td>cebuano (3), tagalog (3), english (5)</td>\n",
       "      <td>...</td>\n",
       "      <td>cebuano (3), tagalog (3)</td>\n",
       "      <td>False</td>\n",
       "      <td>Agustin</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Low-Resource</td>\n",
       "      <td>Both class 3</td>\n",
       "      <td>Ahmad</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scopus</td>\n",
       "      <td>trieu, hai-long (56051704500); nguyen, le-minh...</td>\n",
       "      <td>enhancing pivot translation using grammatical ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>pivot translation can be one of the solutions ...</td>\n",
       "      <td>factored models; lemma forms; part-of-speech t...</td>\n",
       "      <td>computational linguistics; computer aided lang...</td>\n",
       "      <td>low-resource, low resource</td>\n",
       "      <td>morpholog</td>\n",
       "      <td>indonesian (3), malay (3), vietnamese (4)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Agustin</td>\n",
       "      <td>False</td>\n",
       "      <td>Not Low-Resource</td>\n",
       "      <td>None are class 2 or lower, but the authors des...</td>\n",
       "      <td>Ahmad</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        database                                            authors  \\\n",
       "0         scopus  rikters, matiss (57190290578); pinnis, marcis ...   \n",
       "1  acl-anthology  [baliber, renz iver, cheng, charibeth, adlaon,...   \n",
       "2         scopus  trieu, hai-long (56051704500); nguyen, le-minh...   \n",
       "\n",
       "                                               title  year  \\\n",
       "0             advancing estonian machine translation  2018   \n",
       "1  bridging philippine languages with multilingua...  2020   \n",
       "2  enhancing pivot translation using grammatical ...  2018   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  in this paper, we present tildes work on boost...   \n",
       "1  the philippines is home to more than 150 langu...   \n",
       "2  pivot translation can be one of the solutions ...   \n",
       "\n",
       "                                     author keywords  \\\n",
       "0  morphologically rich languages; multi-way mach...   \n",
       "1                                                NaN   \n",
       "2  factored models; lemma forms; part-of-speech t...   \n",
       "\n",
       "                                      index keywords  \\\n",
       "0  computational linguistics; computer aided lang...   \n",
       "1                                                NaN   \n",
       "2  computational linguistics; computer aided lang...   \n",
       "\n",
       "                 low resource morphology  \\\n",
       "0               less resource  morpholog   \n",
       "1                low-resource  morpholog   \n",
       "2  low-resource, low resource  morpholog   \n",
       "\n",
       "                                   languages  ...             languages old  \\\n",
       "0                               estonian (3)  ...              estonian (3)   \n",
       "1      cebuano (3), tagalog (3), english (5)  ...  cebuano (3), tagalog (3)   \n",
       "2  indonesian (3), malay (3), vietnamese (4)  ...                       NaN   \n",
       "\n",
       "  status reviewer 1 reviewer 1 verdict  reviewer 1 reason  \\\n",
       "0  False    Agustin              False   Not Low-Resource   \n",
       "1  False    Agustin              False   Not Low-Resource   \n",
       "2  False    Agustin              False   Not Low-Resource   \n",
       "\n",
       "                                    reviewer 1 notes reviewer 2  \\\n",
       "0  Estonian is level 3, not 2 or lower, and only ...      Ahmad   \n",
       "1                                       Both class 3      Ahmad   \n",
       "2  None are class 2 or lower, but the authors des...      Ahmad   \n",
       "\n",
       "  reviewer 2 verdict  reviewer 2 reason reviewer 2 notes  \n",
       "0                0.0                NaN              NaN  \n",
       "1                0.0                NaN              NaN  \n",
       "2                1.0                NaN              NaN  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_data_path():\n",
    "    return Path().resolve().parents[0]\n",
    "\n",
    "\n",
    "file_path = \"data/dataset-updated.xlsx\"\n",
    "data_path = get_data_path() / file_path\n",
    "\n",
    "df = pd.read_excel(data_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract:\n",
      "in this paper, we present tildes work on boosting the output quality and availability of estonian machine translation systems, focusing mostly on the less resourced and morphologically complex language pairs between estonian and russian.we describe our efforts on collecting parallel and monolingual data for the development of better neural machine translation models, as well as experiments with various model architectures with the goal to find the best performing model for our data.we attain state of the art mt results by training a multi way transformer model that improves the quality by up to +3.27 bleu points over the baseline system.we also provide a publicly available translation service via a mobile phone application. (c) 2018 the authors and ios press.\n",
      "\n",
      "### Step-by-Step Analysis and Reasoning\n",
      "\n",
      "1. **Challenges of morphological complexity**  \n",
      "   Answer: The abstract mentions that the focus is on \"less resourced and morphologically complex language pairs\" between Estonian and Russian, indicating that morphological complexity poses challenges in the context of machine translation.  \n",
      "   Evidence: The phrase \"less resourced and morphologically complex language pairs\" suggests that the complexity of morphology is a challenge for machine translation systems.\n",
      "\n",
      "2. **Proposed techniques**  \n",
      "   Answer: The abstract describes efforts in \"collecting parallel and monolingual data\" and experimenting with \"various model architectures\" to improve machine translation systems.  \n",
      "   Evidence: The mention of \"collecting parallel and monolingual data\" and \"experiments with various model architectures\" indicates the techniques being employed to address the challenges in machine translation.\n",
      "\n",
      "3. **Morphology-aware techniques**  \n",
      "   Answer: The abstract does not specifically mention morphology-aware techniques such as subword modeling or morphological analyzers.  \n",
      "   Evidence: There is no reference to these specific techniques in the abstract, which limits the ability to provide an answer.\n",
      "\n",
      "4. **Specific findings per morphological typology (e.g., polysynthetic, agglutinative, fusional)**  \n",
      "   Answer: The abstract does not provide specific findings, challenges, or proposed solutions for different morphological typologies such as polysynthetic, agglutinative, or fusional languages.  \n",
      "   Evidence: The abstract focuses on Estonian and Russian without detailing findings related to different morphological typologies.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Summary (REASONING):\n",
      "The abstract provides insights into the challenges posed by morphological complexity in low-resource language contexts, specifically between Estonian and Russian. It highlights the techniques of data collection and experimentation with model architectures to improve machine translation. However, it lacks specific information on morphology-aware techniques and does not address findings related to different morphological typologies. This indicates a gap in the information regarding how various morphological complexities are handled in machine translation systems.\n"
     ]
    }
   ],
   "source": [
    "# get first abstract\n",
    "abstract = df[\"abstract\"].iloc[0]\n",
    "\n",
    "research_questions = \"\"\"\\\n",
    "1. What challenges do varying degrees of morphological complexity (e.g., isolating, fusional, agglutinative, and polysynthetic languages) pose to machine translation systems in a low-resource language context? \n",
    "2. What techniques (e.g., rule-based methods, statistical models, or neural architectures) have been proposed to address these challenges?\n",
    "3. How do morphology-aware techniques (e.g. subword modeling, morphological analyzers) compare in effectiveness for low-resource machine translation? \n",
    "4. What are the specific findings, challenges, and proposed solutions and results for machine translation of languages in each different morphological typology (polysynthetic, agglutinative, fusional)?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Chain-of-Thought prompt\n",
    "def chain_of_thought_prompt(abstract: str, research_questions: str):\n",
    "    prompt = f\"\"\"\\\n",
    "    You are an expert researcher. Your task is to analyze the abstract below and use its content to answer a set of specific research questions.\n",
    "\n",
    "    ---\n",
    "\n",
    "    Abstract:\n",
    "    {abstract}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Research Questions:\n",
    "    {research_questions}\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Follow these steps carefully:\n",
    "\n",
    "    Step 1: Read and Understand the Abstract\n",
    "    Carefully read the abstract to extract all relevant information, including stated challenges, techniques used, comparisons, findings, and specific language typologies.\n",
    "\n",
    "    Step 2: Analyze the Abstract for Each Research Question\n",
    "    For each question below, extract answers **only if the abstract provides evidence or insights**. If the abstract does not include the required information, state that explicitly.\n",
    "\n",
    "    Step 3: Provide a Structured Answer with Reasoning\n",
    "    For each research question, write:\n",
    "    - Answer: A direct response based on the abstract.\n",
    "    - Evidence: A brief explanation or citation of the part of the abstract that supports your answer.\n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Return your response in the following format:\n",
    "\n",
    "    ### Step-by-Step Analysis and Reasoning\n",
    "\n",
    "    1. Challenges of morphological complexity\n",
    "    Answer:  \n",
    "    Evidence:  \n",
    "\n",
    "    2. Proposed techniques\n",
    "    Answer:  \n",
    "    Evidence:  \n",
    "\n",
    "    3. Morphology-aware techniques**  \n",
    "    Answer:  \n",
    "    Evidence:  \n",
    "\n",
    "    4. Specific findings per morphological typology (e.g., polysynthetic, agglutinative, fusional)\n",
    "    Answer:  \n",
    "    Evidence:  \n",
    "\n",
    "    ---\n",
    "\n",
    "    ### Final Summary (REASONING):\n",
    "    Briefly summarize what the abstract reveals overall in relation to the research questions. Note any gaps or limitations in the information provided.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# query ChatGPT\n",
    "def workflow(\n",
    "    model,\n",
    "    abstract,\n",
    "    temperature: float = 0.5,\n",
    "    max_tokens: int = 250,\n",
    "):\n",
    "    prompt = chain_of_thought_prompt(abstract, research_questions)\n",
    "    response = prompt_gpt(model, prompt, temperature, max_tokens)\n",
    "    return response\n",
    "\n",
    "\n",
    "# response = prompt_gpt(model, abstract)\n",
    "model = \"gpt-4o-mini\"\n",
    "response = workflow(model, abstract, temperature=0, max_tokens=500)\n",
    "print(f\"Abstract:\\n{abstract}\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_output_and_reasoning(result: str) -> tuple:\n",
    "    # Use re.DOTALL to make '.' match newline characters\n",
    "    match = re.search(r\"OUTPUT:\\s*\\**(.*?)\\**\\s*REASONING:\\s*(.*)\", result, re.DOTALL)\n",
    "    if match:\n",
    "        output_text = match.group(1).replace(\"\\n\", \" \").strip()\n",
    "        reasoning_text = match.group(2).replace(\"\\n\", \" \").strip()\n",
    "        return output_text, reasoning_text\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, reasoning = extract_output_and_reasoning(response)\n",
    "print(output)\n",
    "print(reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append({\"abstract\": abstract, \"output\": output, \"reasoning\": reasoning})\n",
    "\n",
    "df_output = pd.DataFrame(data)\n",
    "df_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = df[\"abstract\"].tolist()\n",
    "subset_abstracts = abstract_list[:10]\n",
    "\n",
    "# process a subset of abstracts\n",
    "for idx, abstract in enumerate(subset_abstracts):\n",
    "    response = workflow(model, abstract, temperature=0, max_tokens=250)\n",
    "    output, reasoning = extract_output_and_reasoning(response)\n",
    "    data.append({\"abstract\": abstract, \"output\": output, \"reasoning\": reasoning})\n",
    "    if idx % 1 == 0:\n",
    "        print(f\"processed {idx}/{len(subset_abstracts)} abstracts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(data)\n",
    "df_output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[\"reasoning\"].iloc[2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
